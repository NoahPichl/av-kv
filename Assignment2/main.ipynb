{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import ObjectDetection.Open3D_ML.ml3d as _ml3d\n",
    "import ObjectDetection.Open3D_ML.ml3d.torch as tml3d\n",
    "from ObjectDetection.TaskDataset import TaskDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D Object Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosen Model\n",
    "\n",
    "PointPillars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Setup\n",
    "\n",
    "Implemented most of the dataloading and preprocessing in TaskDataset.py <br>\n",
    "Load model weigths from pointpillars_waymo.pth <br>\n",
    "The model configuration and general pipelinesettings are defined in pointpillars_waymo.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_file = \"/workspaces/AutomotiveVehicles/Assignment2/ObjectDetection/pointpillars_waymo.yml\"\n",
    "cfg =  _ml3d.utils.Config.load_from_file(cfg_file)\n",
    "\n",
    "model = tml3d.models.PointPillars(**cfg.model)\n",
    "dataset = TaskDataset(cfg.dataset)\n",
    "pipeline = tml3d.pipelines.ObjectDetection(model, dataset=dataset, device=\"gpu\", **cfg.pipeline)\n",
    "\n",
    "# download the weights.\n",
    "ckpt_folder = \"./logs/\"\n",
    "os.makedirs(ckpt_folder, exist_ok=True)\n",
    "ckpt_path = ckpt_folder + \"pointpillars_waymo_202211200158utc_seed2_gpu16.pth\"\n",
    "pointpillar_url = \"https://storage.googleapis.com/open3d-releases/model-zoo/pointpillars_waymo_202211200158utc_seed2_gpu16.pth\"\n",
    "if not os.path.exists(ckpt_path):\n",
    "    cmd = \"wget {} -O {}\".format(pointpillar_url, ckpt_path)\n",
    "    os.system(cmd)\n",
    "\n",
    "# load the parameters.\n",
    "pipeline.load_ckpt(ckpt_path=ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easydict import EasyDict as edict\n",
    "from typing import Tuple\n",
    "\n",
    "def pcl_to_bev(pcl:np.ndarray, configs: edict) -> np.ndarray:\n",
    "    \"\"\"Computes the bev map of a given pointcloud. \n",
    "    \n",
    "    For generality, this method can return the bev map of the available \n",
    "    channels listed in '''BEVConfig.VALID_CHANNELS'''. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        pcl (np.ndarray): pointcloud as a numpy array of shape [n_points, m_channles] \n",
    "        configs (Dict): configuration parameters of the resulting bev_map\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        bev_map (np.ndarray): bev_map as numpy array of shape [len(config.channels), configs.bev_height, configs.bev_width ]\n",
    "    \"\"\"\n",
    "    \n",
    "    # remove lidar points outside detection area and with too low reflectivity\n",
    "    mask = np.where((pcl[:, 0] >= configs.lims.x[0]) & (pcl[:, 0] <= configs.lims.x[1]) &\n",
    "                    (pcl[:, 1] >= configs.lims.y[0]) & (pcl[:, 1] <= configs.lims.y[1]) &\n",
    "                    (pcl[:, 2] >= configs.lims.z[0]) & (pcl[:, 2] <= configs.lims.z[1]))\n",
    "    pcl = pcl[mask]\n",
    "\n",
    "    # shift level of ground plane to avoid flipping from 0 to 255 for neighboring pixels\n",
    "    pcl[:, 2] = pcl[:, 2] - configs.lims.z[0]  \n",
    "\n",
    "    # Convert sensor coordinates to bev-map coordinates (center is bottom-middle)\n",
    "    # compute bev-map discretization by dividing x-range by the bev-image height\n",
    "    bev_x_discret = (configs.lims.x[1] - configs.lims.x[0]) / configs.bev_height\n",
    "    bev_y_discret = (configs.lims.y[1] - configs.lims.y[0]) / configs.bev_width\n",
    "    ## transform all metrix x-coordinates into bev-image coordinates    \n",
    "    pcl_cpy = np.copy(pcl)\n",
    "    pcl_cpy[:, 0] = np.int_(np.floor(pcl_cpy[:, 0] / bev_x_discret))\n",
    "    # transform all y-coordinates making sure that no negative bev-coordinates occur\n",
    "    pcl_cpy[:, 1] = np.int_(np.floor(pcl_cpy[:, 1] / bev_y_discret) + (configs.bev_width + 1) / 2) \n",
    "    # Create BEV map\n",
    "    bev_map = np.zeros((3, configs.bev_height, configs.bev_width))\n",
    "    # Compute height and density channel\n",
    "    pcl_height_sorted, counts = sort_and_map(pcl_cpy, 2, return_counts=True)\n",
    "    xs = np.int_(pcl_height_sorted[:, 0])\n",
    "    ys = np.int_(pcl_height_sorted[:, 1])\n",
    "    # Fill height map\n",
    "    normalized_height = pcl_height_sorted[:, 2]/float(np.abs(configs.lims.z[1] - configs.lims.z[0]))\n",
    "    height_map = np.zeros((configs.bev_height + 1, configs.bev_width + 1))\n",
    "    height_map[xs,ys] = normalized_height\n",
    "    \n",
    "    # Fill density map\n",
    "    normalized_density = np.minimum(1.0, np.log(counts + 1) / np.log(64))\n",
    "    density_map = np.zeros((configs.bev_height + 1, configs.bev_width + 1))\n",
    "    density_map[xs,ys] = normalized_density\n",
    "\n",
    "    # Compute intesity channel\n",
    "    pcl_cpy[pcl_cpy[:,3]>configs.lims.intensity[1],3] = configs.lims.intensity[1]\n",
    "    pcl_cpy[pcl_cpy[:,3]<configs.lims.intensity[0],3] = configs.lims.intensity[0]\n",
    "    \n",
    "    pcl_int_sorted, _ = sort_and_map(pcl_cpy, 3, return_counts=False)\n",
    "    xs = np.int_(pcl_int_sorted[:, 0])\n",
    "    ys = np.int_(pcl_int_sorted[:, 1])\n",
    "    normalized_int = pcl_int_sorted[:, 3]/(np.amax(pcl_int_sorted[:, 3])-np.amin(pcl_int_sorted[:, 3]))\n",
    "    intensity_map = np.zeros((configs.bev_height + 1, configs.bev_width + 1))\n",
    "    intensity_map[xs,ys] = normalized_int\n",
    "    \n",
    "    # Fill BEV \n",
    "    bev_map[2,:,:] = density_map[:configs.bev_height, :configs.bev_width]\n",
    "    bev_map[1,:,:] = height_map[:configs.bev_height, :configs.bev_width]\n",
    "    bev_map[0,:,:] = intensity_map[:configs.bev_height, :configs.bev_width]\n",
    " \n",
    "    return bev_map\n",
    "\n",
    "def sort_and_map(pcl: np.ndarray, channel_index: int, return_counts:bool=False) ->Tuple[np.ndarray,np.ndarray]:\n",
    "    \"\"\"Function to re-arrange elements in poincloud by sorting first by x, then y, then -channel.\n",
    "    This function allows users to map a pointcloud channel to a top view image (in z axis) of that channel.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        pcl (np.ndarray): Input pointcloud of of shape [n_points, m_channles]\n",
    "        channel_index (int): Index of channel to take into account as third factor, \n",
    "                             when sorting the pointcloud.\n",
    "        return_counts (bool): True to return the counts on points per cell. Used for density channel\n",
    "    Returns\n",
    "     ----------\n",
    "       channel_map (np.ndarray): [description]\n",
    "       counts (np.ndarray): [description]\n",
    "       \n",
    "    \"\"\"\n",
    "\n",
    "    idx= np.lexsort((-pcl[:, channel_index], pcl[:, 1], pcl[:, 0]))\n",
    "    pcl_sorted = pcl[idx]\n",
    "    counts = None\n",
    "    # extract all points with identical x and y such that only the maximum value of the channel is kept\n",
    "    if return_counts:\n",
    "        _, indices, counts = np.unique(pcl_sorted[:, 0:2], axis=0, return_index=True, return_counts=return_counts)\n",
    "    else:\n",
    "        _, indices = np.unique(pcl_sorted[:, 0:2], axis=0, return_index=True)\n",
    "    return (pcl_sorted[indices], counts)\n",
    "\n",
    "def show_bev_map(bev_map: np.ndarray) -> None:\n",
    "    \"\"\"Function to show bev_map as an RGB image\n",
    "\n",
    "    By default, the image will only show the 3 first channels of `bev_map`. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        bev_map (np.ndarray): bev_map as numpy array of shape `[len(config.channels), configs.bev_height, configs.bev_width ]` \n",
    "    \"\"\"\n",
    "    bev_image: np.ndarray =  (np.swapaxes(np.swapaxes(bev_map,0,1),1,2)*255).astype(np.uint8)\n",
    "    mask: np.ndarray = np.zeros_like(bev_image[:,:,0])\n",
    "\n",
    "\n",
    "    height_image = Image.fromarray(np.dstack((bev_image[:, :, 0],mask,mask)))\n",
    "    den_image = Image.fromarray(np.dstack((mask,bev_image[:, :, 1],mask)))\n",
    "    int_image = Image.fromarray(np.dstack((mask,mask,bev_image[:, :, 2])))\n",
    "\n",
    "    int_image.show()\n",
    "    den_image.show()\n",
    "    height_image.show()\n",
    "    Image.fromarray(bev_image).show()\n",
    "\n",
    "\n",
    "configs = edict()\n",
    "configs.lims = edict()\n",
    "configs.lims.x = [0, 50]\n",
    "configs.lims.y = [-25, 25]\n",
    "configs.lims.z = [-1.5, 3]\n",
    "configs.lims.intensity = [0, 1.0]\n",
    "configs.bev_height = 640\n",
    "configs.bev_width = 640\n",
    "\n",
    "bev_map = pcl_to_bev(pcl, configs)\n",
    "show_bev_map(bev_map)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Multi Object Tracking\n",
    "\n",
    "The whole tracking pipeline is broken down into several steps which are described next:\n",
    "\n",
    "1. Data Collection\n",
    "    * Sensors: The pipeline begins by collecting data from various sensors:\n",
    "        * Cameras provide 2D images from which object positions are inferred (x, y coordinates in the image plane).\n",
    "        * Lidars give 3D spatial data, typically used to determine precise distances and relative positions of objects in 3D space, but here simplified to 2D (x, y).\n",
    "\n",
    "2. Data Extraction\n",
    "    * Detection Extraction: From the raw sensor outputs, specific features or measurements are extracted:\n",
    "        * From Camera: The bounding boxes of the detections are used to get the center points.\n",
    "        * From Lidar: Points are identified to localize objects in 2D space (x and y).\n",
    "\n",
    "3. Initialization of Tracking\n",
    "    * Track Creation: When new objects are detected and no existing tracks are found to correspond to these new detections, new tracks are initialized:\n",
    "        * Each track is equipped with a Kalman Filter initialized to the detected position, with an assumed (zero) initial velocity.\n",
    "        * The state covariance is set high to reflect initial uncertainty about the state of each new track.\n",
    "\n",
    "4. Prediction\n",
    "    * State Prediction: For each existing track, the Kalman Filter predicts the next state based on the previous state and the model of motion (defined by the transition matrix F):\n",
    "        * This prediction extends the understanding of where each tracked object might move in the next time step, accounting for inherent uncertainties and the dynamics of motion (like velocity).\n",
    "\n",
    "5. Data Association\n",
    "    * Matching Detections to Tracks: This crucial step determines which detections correspond to which existing tracks:\n",
    "        * A cost matrix is constructed based on the distance (e.g., Euclidean) between predicted positions of tracks and new detections.\n",
    "\n",
    "6. Update\n",
    "    * State Update: Once detections are associated with tracks, the Kalman Filter for each track updates its state with the new measurement:\n",
    "        * The update adjusts the tracked position and velocity to more closely reflect the observed data, improving the accuracy of the track.\n",
    "\n",
    "7. Maintenance\n",
    "    * Track Management: Tracks are managed continuously to ensure they remain relevant and accurate:\n",
    "        * Pruning: Tracks that have not been updated for several cycles (due to missing detections) are removed to prevent clutter and false tracking.\n",
    "        * Creation: New tracks are created for detections that weren't associated with any existing track.\n",
    "\n",
    "8. Visualization\n",
    "    * Output Visualization: The current state of tracks is visualized on the video or image stream to provide real-time feedback on the tracking performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install OpenCV for visualization\n",
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "import tools.frame_pb2 as fpb\n",
    "import tools.dataset_tools as dataset_tools\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from matplotlib.patches import Rectangle, Circle\n",
    "import cv2\n",
    "\n",
    "## Constants\n",
    "max_allowed_missed_frames = 5\n",
    "update_tracks_threshold = 30\n",
    "cost_matrix = np.zeros((1, 1), dtype=float)\n",
    "video_path = \"./out.mp4\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_frame(filename):\n",
    "    frame = dataset_tools.read_frame(filename)\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_detections_from_camera(camera):\n",
    "    detections = []\n",
    "    for detection in camera.detections:\n",
    "        # Assuming bbox is [x0, y0, width, height]\n",
    "        # Convert bbox to [x_center, y_center]\n",
    "        x_center = detection.bbox[0] #- detection.bbox[2] / 2\n",
    "        y_center = detection.bbox[1] #- detection.bbox[3] / 2\n",
    "        detections.append(np.array([x_center, y_center]))\n",
    "    return detections\n",
    "\n",
    "def extract_detections_from_lidar(lidar):\n",
    "    detections = []\n",
    "    for detection in lidar.detections:\n",
    "        # Assuming pos is [x, y, z]\n",
    "        # Use x and y for 2D tracking\n",
    "        detections.append(np.array([detection.pos[0], detection.pos[1]]))\n",
    "    return detections\n",
    "\n",
    "def extract_detections(frame):\n",
    "    detections = []\n",
    "    for camera in frame.cameras:\n",
    "        detections.extend(extract_detections_from_camera(camera))\n",
    "    for lidar in frame.lidars:\n",
    "        detections.extend(extract_detections_from_lidar(lidar))\n",
    "    return detections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kalman Filter Implementation\n",
    "\n",
    "The Kalman Filter implemented in the code is designed for tracking the position and velocity of an object in two dimensions (2D). It estimates these properties over time based on noisy observations, which might typically come from sensors like cameras or lidar. Here's a breakdown of how each part of the Kalman Filter works and its role in the tracking process:\n",
    "\n",
    "#### Initialization\n",
    "* Time Step (dt): This is the time interval between filter updates, which influences the prediction of the next state.\n",
    "* State Transition Model (F): The matrix F predicts the next state based on the current state. It considers both position and velocity for each dimension (x and y), integrating the effect of the time step on the position due to velocity.\n",
    "* Process Noise Covariance (Q): The matrix Q represents the process noise covariance, accounting for the uncertainty in the model prediction. It helps the filter to be robust to the inherent inaccuracies of the motion model, particularly by scaling with the square of the time step (dt), reflecting increased uncertainty over longer periods.\n",
    "* Measurement Model (H): The matrix H maps the predicted state to the measurement space. In this case, it indicates that the measurements directly correspond to position (x, y) and do not include velocity.\n",
    "* Observation Noise Covariance (R): The matrix R quantifies the expected noise in the measurements. A larger R decreases the weight given to the observations during updates, making the filter trust its model prediction more relative to the noisy observations.\n",
    "\n",
    "#### Functions\n",
    "##### initialize\n",
    "Initializes the filter's state vector (x) with the provided initial position and an optional velocity (defaulting to zero if not provided). This method also sets the initial estimate of the uncertainty (P, the covariance matrix) to a high value, indicating that initially, the exact state of the system is less certain.\n",
    "\n",
    "##### predict\n",
    "Uses the state transition model (F) to predict the next state based on the current state and updates the covariance matrix (P) to reflect the increased uncertainty due to the inherent error in the model. This step does not incorporate new measurement data but extrapolates the current state estimate to the next time step.\n",
    "\n",
    "##### update\n",
    "Incorporates new measurement data into the state estimate:\n",
    "* Measurement Residual (y): The difference between the actual measurement and what the model predicted for this measurement.\n",
    "* Residual Covariance (S): Represents the uncertainty of the residual.\n",
    "* Kalman Gain (K): Determines how much the new measurement should influence the updated state. A higher gain places more weight on the new measurement relative to the predicted state.\n",
    "* Updates the state estimate (x) by applying the Kalman Gain to the residual.\n",
    "* Updates the estimate of the state uncertainty (P) to reflect the reduced uncertainty after incorporating the new measurement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Kalman Filter\n",
    "class KalmanFilter:\n",
    "    def __init__(self, dt, process_noise_std, measurement_noise_std):\n",
    "        # Time step\n",
    "        self.dt = dt\n",
    "\n",
    "        # State transition model matrix (F)\n",
    "        self.F = np.array([\n",
    "            [1, 0, dt, 0],  # Position x and velocity influence\n",
    "            [0, 1, 0, dt],  # Position y and velocity influence\n",
    "            [0, 0, 1, 0],   # Velocity x\n",
    "            [0, 0, 0, 1]    # Velocity y\n",
    "        ])\n",
    "\n",
    "        # Process noise covariance matrix (Q)\n",
    "        q = process_noise_std**2\n",
    "        self.Q = q * np.array([\n",
    "            [dt**4/4, 0, dt**3/2, 0],\n",
    "            [0, dt**4/4, 0, dt**3/2],\n",
    "            [dt**3/2, 0, dt**2, 0],\n",
    "            [0, dt**3/2, 0, dt**2]\n",
    "        ])\n",
    "\n",
    "        # Measurement model matrix (H)\n",
    "        self.H = np.array([\n",
    "            [1, 0, 0, 0],  # Measuring position x\n",
    "            [0, 1, 0, 0]   # Measuring position y\n",
    "        ])\n",
    "\n",
    "        # Observation noise covariance matrix (R)\n",
    "        r = measurement_noise_std**2\n",
    "        self.R = r * np.eye(2)\n",
    "\n",
    "        # Initial state estimate and covariance matrix (not initialized yet)\n",
    "        self.x = None\n",
    "        self.P = None\n",
    "\n",
    "    def initialize(self, initial_position, initial_velocity=None):\n",
    "        \"\"\" Initialize the state and covariance matrices \"\"\"\n",
    "        # print(f\"Initializing Kalman Filter with initial position: {initial_position}\")\n",
    "        if initial_velocity is None:\n",
    "            initial_velocity = [0, 0]  # Assume initial velocity is 0 if not provided\n",
    "        self.x = np.array([initial_position[0], initial_position[1], initial_velocity[0], initial_velocity[1]])\n",
    "        self.P = np.eye(4)  # High initial uncertainty\n",
    "\n",
    "    def predict(self):\n",
    "        \"\"\" Predict the state and covariance after time dt \"\"\"\n",
    "        # print(f'Predicting Kalman Filter with dt = {self.dt}')\n",
    "        self.x = np.dot(self.F, self.x)\n",
    "        self.P = np.dot(self.F, np.dot(self.P, self.F.T)) + self.Q\n",
    "\n",
    "    def update(self, measurement):\n",
    "        \"\"\" Update the state by a new measurement \"\"\"\n",
    "        # print(f'Updating Kalman Filter with measurement = {measurement}')\n",
    "        z = np.array(measurement)\n",
    "        y = z - np.dot(self.H, self.x)  # Measurement residual\n",
    "        S = np.dot(self.H, np.dot(self.P, self.H.T)) + self.R  # Residual covariance\n",
    "        K = np.dot(self.P, np.dot(self.H.T, np.linalg.inv(S)))  # Kalman gain\n",
    "        self.x = self.x + np.dot(K, y)\n",
    "        I = np.eye(self.F.shape[0])\n",
    "        self.P = np.dot((I - np.dot(K, self.H)), self.P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracker Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tracker Class\n",
    "class Tracker:\n",
    "    def __init__(self):\n",
    "        self.tracks = []\n",
    "        self.next_id = 1\n",
    "\n",
    "    def update_tracks(self, detections):\n",
    "        # Call predict on all existing tracks\n",
    "        for track in self.tracks:\n",
    "            track['kf'].predict()\n",
    "\n",
    "        if not self.tracks:  # If no tracks exist, initialize tracks with each detection\n",
    "            for det in detections:\n",
    "                kf = KalmanFilter(dt=0.1, process_noise_std=1.0, measurement_noise_std=1.0)\n",
    "                kf.initialize(det)\n",
    "                self.tracks.append({'id': self.next_id, 'kf': kf, 'last_detected': 0})\n",
    "                self.next_id += 1\n",
    "            return\n",
    "\n",
    "        # Association step\n",
    "        cost_matrix = np.zeros((len(detections), len(self.tracks)))\n",
    "        for d_index, detection in enumerate(detections):\n",
    "            for t_index, track in enumerate(self.tracks):\n",
    "                predicted_position = track['kf'].x[:2]\n",
    "                cost_matrix[d_index, t_index] = np.linalg.norm(detection - predicted_position)\n",
    "\n",
    "        row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "        all_detections = set(range(len(detections)))\n",
    "        all_tracks = set(range(len(self.tracks)))\n",
    "\n",
    "        used_detections = set(row_ind)\n",
    "        unused_detections = all_detections - used_detections\n",
    "        used_tracks = set(col_ind)\n",
    "        unused_tracks = all_tracks - used_tracks\n",
    "\n",
    "        # Update associated tracks\n",
    "        for d, t in zip(row_ind, col_ind):\n",
    "            self.tracks[t]['kf'].update(detections[d])\n",
    "            self.tracks[t]['last_detected'] = 0\n",
    "\n",
    "        # Create new tracks for unmatched detections\n",
    "        for d in unused_detections:\n",
    "            kf = KalmanFilter(dt=0.1, process_noise_std=1.0, measurement_noise_std=1.0)\n",
    "            kf.initialize(detections[d])\n",
    "            self.tracks.append({'id': self.next_id, 'kf': kf, 'last_detected': 0})\n",
    "            self.next_id += 1\n",
    "\n",
    "        # Increase age of all tracks and remove old ones\n",
    "        self.prune_tracks()\n",
    "\n",
    "    def prune_tracks(self):\n",
    "        self.tracks = [track for track in self.tracks if track['last_detected'] < 5]\n",
    "        for track in self.tracks:\n",
    "            track['last_detected'] += 1\n",
    "\n",
    "    # def visualize_tracks(self, frame_index):\n",
    "    #     plt.figure(figsize=(10, 8))\n",
    "    #     for track in self.tracks:\n",
    "    #         plt.plot(track['kf'].x[0], track['kf'].x[1], 'bo')\n",
    "    #         plt.text(track['kf'].x[0], track['kf'].x[1], str(track['id']), color=\"red\")\n",
    "    #     plt.title(f\"Frame {frame_index}\")\n",
    "    #     plt.grid(True)\n",
    "    #     plt.show()\n",
    "    \n",
    "    def visualize_tracks_img(self, image, detections, frame_index):\n",
    "        \"\"\"\n",
    "        Visualize the current tracks and detections on the image.\n",
    "\n",
    "        :param image: The image array (numpy array).\n",
    "        :param tracks: List of current tracks with their Kalman Filter state estimates.\n",
    "        :param detections: List of detections as (x_center, y_center) used for initial track positions.\n",
    "        :param frame_index: Index of the current frame for title display.\n",
    "        \"\"\"\n",
    "        fig, ax = plt.subplots(1, figsize=(10, 8))\n",
    "        ax.imshow(image)\n",
    "        \n",
    "        # Draw detections as rectangles\n",
    "        for det in detections:\n",
    "            rect = Rectangle((det[0] - 15, det[1] - 15), 30, 30, linewidth=2, edgecolor='blue', facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "        \n",
    "        # Draw tracked positions as circles\n",
    "        for track in self.tracks:\n",
    "            x, y = track['kf'].x[0], track['kf'].x[1]\n",
    "            circle = Circle((x, y), 15, color='red', fill=False, linewidth=2)\n",
    "            ax.add_patch(circle)\n",
    "            ax.text(x, y, str(track['id']), color=\"white\", fontsize=12)\n",
    "\n",
    "        plt.title(f\"Frame {frame_index}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    def visualize_tracks(self, image, detections, frame_index):\n",
    "        \"\"\"\n",
    "        Visualize the current tracks and detections on the image using OpenCV.\n",
    "\n",
    "        :param image: The image array (numpy array).\n",
    "        :param detections: List of detections as (x_center, y_center) used for initial track positions.\n",
    "        :param tracks: List of current tracks with their Kalman Filter state estimates.\n",
    "        :param frame_index: Index of the current frame for title display.\n",
    "        \"\"\"\n",
    "        # Convert image from RGB to BGR (OpenCV uses BGR by default)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # Draw detections as rectangles\n",
    "        for det in detections:\n",
    "            top_left = (int(det[0] - 15), int(det[1] - 15))\n",
    "            bottom_right = (int(det[0] + 15), int(det[1] + 15))\n",
    "            cv2.rectangle(image, top_left, bottom_right, (255, 0, 0), 2)  # Blue rectangles\n",
    "\n",
    "        # Draw tracked positions as circles\n",
    "        for track in self.tracks:\n",
    "            center = (int(track['kf'].x[0]), int(track['kf'].x[1]))\n",
    "            cv2.circle(image, center, 15, (0, 0, 255), 2)  # Red circles\n",
    "            cv2.putText(image, str(track['id']), (center[0] + 10, center[1] + 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "\n",
    "        return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Process Frames\n",
    "def create_video_writer(frame_size, output_path='output.mp4'):\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, 20.0, frame_size)\n",
    "    return out\n",
    "\n",
    "def show_image(frame):\n",
    "    camera = frame.cameras[0]\n",
    "\n",
    "    img = dataset_tools.decode_img(camera)\n",
    "\n",
    "    image = Image.fromarray(img)\n",
    "    image.show()\n",
    "    return img  # Return the image array for further processing\n",
    "\n",
    "def show_video(frame):\n",
    "    camera = frame.cameras[0]\n",
    "    img = dataset_tools.decode_img(camera)\n",
    "    image = Image.fromarray(img)\n",
    "    image.show()\n",
    "    return np.array(image)  # Ensure this is RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "# def process_frames(directory, tracker):\n",
    "#     for i in range(len(os.listdir(directory))):\n",
    "#         filename = f'frame_{i}.pb'\n",
    "#         frame = load_frame(os.path.join(directory, filename))\n",
    "#         image = show_image(frame)  # Ensure this returns the image array\n",
    "#         # image = show_video(frame)  # Ensure this returns the image array\n",
    "#         detections = extract_detections(frame)\n",
    "#         tracker.update_tracks(detections)\n",
    "#         tracker.visualize_tracks_img(image, detections, frame.id)\n",
    "#         # tracker.visualize_tracks(image, detections, frame.id)\n",
    "\n",
    "def process_frames(directory, tracker):\n",
    "    video_writer = None\n",
    "    first_frame = True\n",
    "\n",
    "    for i in range(len(os.listdir(directory))):\n",
    "        filename = f'frame_{i}.pb'\n",
    "        frame = load_frame(os.path.join(directory, filename))\n",
    "        image = show_video(frame)  # This should now return an RGB image array\n",
    "        detections = extract_detections(frame)\n",
    "        tracker.update_tracks(detections)\n",
    "        \n",
    "        processed_image = tracker.visualize_tracks(image, detections, frame.id)\n",
    "        \n",
    "        # visualize image on processing\n",
    "        img = show_image(frame)\n",
    "        tracker.visualize_tracks_img(img, detections, frame.id)\n",
    "        \n",
    "        if first_frame:\n",
    "            video_writer = create_video_writer((processed_image.shape[1], processed_image.shape[0]))\n",
    "            first_frame = False\n",
    "        \n",
    "        video_writer.write(processed_image)\n",
    "\n",
    "    video_writer.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = Tracker()\n",
    "process_frames(\"../Dataset/data_2\", tracker)\n",
    "\n",
    "# Remember to close OpenCV windows once done\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
